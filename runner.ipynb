{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924f1166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found court categories: ['Supreme Court of India', 'Supreme Court - Daily Orders', 'Allahabad High Court', 'Andhra HC (Pre-Telangana)', 'Andhra Pradesh High Court - Amravati', 'Bombay High Court', 'Calcutta High Court', 'Calcutta High Court (Appellete Side)', 'Chattisgarh High Court', 'Delhi High Court', 'Delhi High Court - Orders', 'Gauhati High Court', 'Gujarat High Court', 'Himachal Pradesh High Court', 'Jammu & Kashmir High Court', 'Jammu & Kashmir High Court - Srinagar Bench', 'Jharkhand High Court', 'Karnataka High Court', 'Kerala High Court', 'Madhya Pradesh High Court', 'Manipur High Court', 'Meghalaya High Court', 'Madras High Court', 'Orissa High Court', 'Patna High Court', 'Patna High Court - Orders', 'Punjab-Haryana High Court', 'Rajasthan High Court - Jaipur', 'Rajasthan High Court - Jodhpur', 'Sikkim High Court', 'Uttarakhand High Court', 'Tripura High Court', 'Telangana High Court', 'Delhi District Court', 'Bangalore District Court', 'Appellate Tribunal For Electricity', 'Authority Tribunal', 'Central Administrative Tribunal', 'Customs, Excise and Gold Tribunal', 'Central Electricity Regulatory Commission', 'Central Information Commission', 'Company Law Board', 'Consumer Disputes Redressal', 'Copyright Board', 'Debt Recovery Appellate Tribunal', 'National Green Tribunal', 'Competition Commission of India', 'Intellectual Property Appellate Board', 'Income Tax Appellate Tribunal', 'Monopolies and Restrictive Trade Practices Commission', 'Securities Appellate Tribunal', 'State Taxation Tribunal', 'Telecom Disputes Settlement Tribunal', 'Trademark Tribunal', 'Custom, Excise & Service Tax Tribunal', 'National Company Law Appellate Tribunal', 'Law Commission Report', 'Constituent Assembly Debates', 'Lok Sabha Debates', 'Rajya Sabha Debates']\n",
      "\n",
      "ðŸ“‚ Scraping Supreme Court of India\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Supreme Court - Daily Orders\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Allahabad High Court\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Andhra HC (Pre-Telangana)\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Andhra Pradesh High Court - Amravati\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Bombay High Court\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Calcutta High Court\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Calcutta High Court (Appellete Side)\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Chattisgarh High Court\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Delhi High Court\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Delhi High Court - Orders\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Gauhati High Court\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Gujarat High Court\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Himachal Pradesh High Court\n",
      "   âœ… Total 0 cases fetched.\n",
      "\n",
      "ðŸ“‚ Scraping Jammu & Kashmir High Court\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m         save_to_csv(court_name, court_cases)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# RUN\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m scrape_all()\n\u001b[0;32m    122\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "Cell \u001b[1;32mIn[10], line 114\u001b[0m, in \u001b[0;36mscrape_all\u001b[1;34m()\u001b[0m\n\u001b[0;32m    112\u001b[0m court_cases \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m YEARS:\n\u001b[1;32m--> 114\u001b[0m     yearly_cases \u001b[38;5;241m=\u001b[39m get_cases_by_year(court_url, year)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m yearly_cases:\n\u001b[0;32m    116\u001b[0m         court_cases\u001b[38;5;241m.\u001b[39mextend(yearly_cases)\n",
      "Cell \u001b[1;32mIn[10], line 65\u001b[0m, in \u001b[0;36mget_cases_by_year\u001b[1;34m(court_url, year)\u001b[0m\n\u001b[0;32m     63\u001b[0m paginated_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcourt_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?year=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&p=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(paginated_url)\n\u001b[1;32m---> 65\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     66\u001b[0m results \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:455\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    448\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    :Usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m            driver.page_source\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET_PAGE_SOURCE)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:352\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    350\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:306\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    304\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    305\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:326\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    323\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 326\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    327\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:135\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m     urlopen_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m body\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_url_methods:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    136\u001b[0m         method,\n\u001b[0;32m    137\u001b[0m         url,\n\u001b[0;32m    138\u001b[0m         fields\u001b[38;5;241m=\u001b[39mfields,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    139\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m    144\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m    145\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:182\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields:\n\u001b[0;32m    180\u001b[0m     url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m urlencode(fields)\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alene\\anaconda3\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "# CONFIG\n",
    "BASE_URL = \"https://indiankanoon.org\"\n",
    "BROWSE_URL = f\"{BASE_URL}/browse/\"\n",
    "DOWNLOAD_DIR = os.path.join(os.getcwd(), \"indiankanoon-case-dump\")\n",
    "YEARS = list(range(1946, 2026))\n",
    "USERNAME = \"sabu.s.alan@gmail.com\"\n",
    "PASSWORD = \"Alan@123\"\n",
    "\n",
    "chromedriver_autoinstaller.install()\n",
    "Path(DOWNLOAD_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_experimental_option(\"prefs\", {\"download.default_directory\": DOWNLOAD_DIR})\n",
    "driver = webdriver.Chrome(options=options)\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "def login():\n",
    "    driver.get(f\"{BASE_URL}/members/login/?nextpage=/\")\n",
    "    wait.until(EC.presence_of_element_located((By.NAME, \"email\"))).send_keys(USERNAME)\n",
    "    driver.find_element(By.NAME, \"passwd\").send_keys(PASSWORD)\n",
    "    driver.find_element(By.XPATH, \"//input[@type='submit']\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "def get_court_category_links():\n",
    "    driver.get(BROWSE_URL)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    info_blocks = soup.find_all('div', class_='info_indian_kanoon')\n",
    "    court_links = {}\n",
    "    for block in info_blocks:\n",
    "        for link in block.find_all('a'):\n",
    "            label = link.text.strip()\n",
    "            court_links[label] = BASE_URL + link['href']\n",
    "    return court_links\n",
    "\n",
    "def get_case_text(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        content = soup.find('div', class_='judgments') or soup.find('pre')\n",
    "        return content.get_text(separator='\\n', strip=True) if content else \"N/A\"\n",
    "    except Exception:\n",
    "        return \"N/A\"\n",
    "\n",
    "def get_cases_by_year(court_url, year):\n",
    "    all_cases = []\n",
    "    page = 0\n",
    "    while True:\n",
    "        paginated_url = f\"{court_url}?year={year}&p={page}\"\n",
    "        driver.get(paginated_url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        results = soup.find_all('div', class_='result')\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        for div in results:\n",
    "            title_tag = div.find(\"a\")\n",
    "            if not title_tag:\n",
    "                continue\n",
    "            title = title_tag.text.strip()\n",
    "            url = BASE_URL + title_tag['href']\n",
    "            snippet = div.find(\"p\", class_=\"snippet\")\n",
    "            snippet_text = snippet.text.strip() if snippet else \"\"\n",
    "            date_span = div.find(\"span\", class_=\"result_date\")\n",
    "            date_text = date_span.text.strip() if date_span else \"\"\n",
    "\n",
    "            print(f\"   ðŸ“„ {year} | {title}\")\n",
    "            full_text = get_case_text(url)\n",
    "\n",
    "            all_cases.append({\n",
    "                \"Title\": title,\n",
    "                \"URL\": url,\n",
    "                \"Snippet\": snippet_text,\n",
    "                \"Date\": date_text,\n",
    "                \"Full_Text\": full_text\n",
    "            })\n",
    "        page += 1\n",
    "        time.sleep(0.5)\n",
    "    return all_cases\n",
    "\n",
    "def save_to_csv(court_name, cases):\n",
    "    if not cases:\n",
    "        return\n",
    "    safe_name = court_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "    file_path = os.path.join(DOWNLOAD_DIR, f\"{safe_name}.csv\")\n",
    "    with open(file_path, \"w\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"Title\", \"URL\", \"Snippet\", \"Date\", \"Full_Text\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(cases)\n",
    "\n",
    "def scrape_all():\n",
    "    login()\n",
    "    court_links = get_court_category_links()\n",
    "    print(\"âœ… Found court categories:\", list(court_links.keys()))\n",
    "\n",
    "    for court_name, court_url in court_links.items():\n",
    "        print(f\"\\nðŸ“‚ Scraping {court_name}\")\n",
    "        court_cases = []\n",
    "        for year in YEARS:\n",
    "            yearly_cases = get_cases_by_year(court_url, year)\n",
    "            if yearly_cases:\n",
    "                court_cases.extend(yearly_cases)\n",
    "        print(f\"   âœ… Total {len(court_cases)} cases fetched.\")\n",
    "        save_to_csv(court_name, court_cases)\n",
    "\n",
    "# RUN\n",
    "scrape_all()\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776d03d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--username USERNAME] [--password PASSWORD]\n",
      "                             [--output OUTPUT] [--start-year START_YEAR]\n",
      "                             [--end-year END_YEAR] [--no-full-text]\n",
      "                             [--delay-min DELAY_MIN] [--delay-max DELAY_MAX]\n",
      "                             [--checkpoint-interval CHECKPOINT_INTERVAL]\n",
      "                             [--retry-limit RETRY_LIMIT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\alene\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3d0c322dd4788d27bf559ead74fc8b2a189f7a15d.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import chromedriver_autoinstaller\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "class IndianKanoonScraper:\n",
    "    def __init__(self, config):\n",
    "        # Configuration\n",
    "        self.base_url = config.get('base_url', \"https://indiankanoon.org\")\n",
    "        self.browse_url = f\"{self.base_url}/browse/\"\n",
    "        self.download_dir = config.get('download_dir', os.path.join(os.getcwd(), \"indiankanoon-case-dump\"))\n",
    "        self.start_year = config.get('start_year', 1946)\n",
    "        self.end_year = config.get('end_year', datetime.now().year)\n",
    "        self.username = config.get('username')\n",
    "        self.password = config.get('password')\n",
    "        self.delay_min = config.get('delay_min', 1)\n",
    "        self.delay_max = config.get('delay_max', 3)\n",
    "        self.checkpoint_file = os.path.join(self.download_dir, \"checkpoint.json\")\n",
    "        self.checkpoint_interval = config.get('checkpoint_interval', 10)\n",
    "        self.retry_limit = config.get('retry_limit', 3)\n",
    "        self.include_full_text = config.get('include_full_text', True)\n",
    "        \n",
    "        # Setup logging\n",
    "        log_file = os.path.join(self.download_dir, \"scraper.log\")\n",
    "        self.setup_logging(log_file)\n",
    "        \n",
    "        # Setup directories\n",
    "        Path(self.download_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Checkpoint data\n",
    "        self.checkpoint = self.load_checkpoint()\n",
    "        \n",
    "        # Stats\n",
    "        self.stats = {\n",
    "            \"cases_scraped\": 0,\n",
    "            \"courts_completed\": 0,\n",
    "            \"years_completed\": 0,\n",
    "            \"errors\": 0,\n",
    "            \"start_time\": time.time(),\n",
    "        }\n",
    "        \n",
    "        # Initialize browser\n",
    "        self._setup_browser()\n",
    "    \n",
    "    def setup_logging(self, log_file):\n",
    "        \"\"\"Set up logging to both file and console\"\"\"\n",
    "        self.logger = logging.getLogger('indiankanoon_scraper')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # File handler\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        self.logger.addHandler(file_handler)\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        self.logger.addHandler(console_handler)\n",
    "    \n",
    "    def _setup_browser(self):\n",
    "        \"\"\"Set up the Chrome browser with appropriate options\"\"\"\n",
    "        self.logger.info(\"Setting up Chrome browser\")\n",
    "        chromedriver_autoinstaller.install()\n",
    "        \n",
    "        options = Options()\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--disable-popup-blocking\")\n",
    "        # Uncomment the next line for headless mode if needed\n",
    "        # options.add_argument(\"--headless\")\n",
    "        options.add_experimental_option(\"prefs\", {\n",
    "            \"download.default_directory\": self.download_dir,\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"plugins.always_open_pdf_externally\": True\n",
    "        })\n",
    "        \n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.wait = WebDriverWait(self.driver, 20)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load checkpoint from file if exists\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'r') as f:\n",
    "                    checkpoint = json.load(f)\n",
    "                self.logger.info(f\"Loaded checkpoint: {checkpoint}\")\n",
    "                return checkpoint\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error loading checkpoint: {e}\")\n",
    "        \n",
    "        # Default checkpoint data\n",
    "        return {\n",
    "            \"courts_completed\": [],\n",
    "            \"current_court\": None,\n",
    "            \"current_court_years_completed\": [],\n",
    "            \"current_year\": None,\n",
    "            \"current_year_page\": 0\n",
    "        }\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save current progress to checkpoint file\"\"\"\n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'w') as f:\n",
    "                json.dump(self.checkpoint, f)\n",
    "            self.logger.info(f\"Checkpoint saved: {self.checkpoint}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving checkpoint: {e}\")\n",
    "    \n",
    "    def random_delay(self):\n",
    "        \"\"\"Sleep for a random time to avoid detection\"\"\"\n",
    "        delay = random.uniform(self.delay_min, self.delay_max)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def login(self):\n",
    "        \"\"\"Log into Indian Kanoon\"\"\"\n",
    "        if not (self.username and self.password):\n",
    "            self.logger.warning(\"No login credentials provided, skipping login\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            self.logger.info(\"Logging in...\")\n",
    "            self.driver.get(f\"{self.base_url}/members/login/?nextpage=/\")\n",
    "            \n",
    "            # Wait for login form\n",
    "            self.wait.until(EC.presence_of_element_located((By.NAME, \"email\")))\n",
    "            \n",
    "            # Enter credentials\n",
    "            self.driver.find_element(By.NAME, \"email\").send_keys(self.username)\n",
    "            self.driver.find_element(By.NAME, \"passwd\").send_keys(self.password)\n",
    "            \n",
    "            # Submit form\n",
    "            self.driver.find_element(By.XPATH, \"//input[@type='submit']\").click()\n",
    "            \n",
    "            # Wait for redirect\n",
    "            self.random_delay()\n",
    "            \n",
    "            # Check if login was successful\n",
    "            if \"members/login\" in self.driver.current_url:\n",
    "                self.logger.error(\"Login failed. Check credentials.\")\n",
    "                return False\n",
    "                \n",
    "            self.logger.info(\"Login successful\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during login: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_court_category_links(self):\n",
    "        \"\"\"Get links to all courts\"\"\"\n",
    "        self.logger.info(\"Getting court category links\")\n",
    "        \n",
    "        # Retry logic for robustness\n",
    "        for attempt in range(self.retry_limit):\n",
    "            try:\n",
    "                self.driver.get(self.browse_url)\n",
    "                self.random_delay()\n",
    "                \n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                info_blocks = soup.find_all('div', class_='info_indian_kanoon')\n",
    "                \n",
    "                court_links = {}\n",
    "                for block in info_blocks:\n",
    "                    for link in block.find_all('a'):\n",
    "                        label = link.text.strip()\n",
    "                        if label:  # Ensure label is not empty\n",
    "                            court_links[label] = self.base_url + link['href']\n",
    "                \n",
    "                self.logger.info(f\"Found {len(court_links)} court categories\")\n",
    "                return court_links\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error getting court links (attempt {attempt+1}): {e}\")\n",
    "                self.random_delay()\n",
    "                \n",
    "        # If we get here, all attempts failed\n",
    "        self.logger.critical(\"Failed to get court categories after multiple attempts\")\n",
    "        return {}\n",
    "    \n",
    "    def get_case_text(self, url):\n",
    "        \"\"\"Extract full text of a case\"\"\"\n",
    "        if not self.include_full_text:\n",
    "            return \"Full text extraction disabled\"\n",
    "            \n",
    "        self.logger.debug(f\"Getting case text from {url}\")\n",
    "        \n",
    "        for attempt in range(self.retry_limit):\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "                self.random_delay()\n",
    "                \n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                \n",
    "                # Try different selectors for the judgment text\n",
    "                content_selectors = [\n",
    "                    ('div', {'class': 'judgments'}),\n",
    "                    ('pre', {}),\n",
    "                    ('div', {'class': 'docsnippet'}),\n",
    "                    ('div', {'id': 'doc_content'})\n",
    "                ]\n",
    "                \n",
    "                for tag, attrs in content_selectors:\n",
    "                    content = soup.find(tag, attrs)\n",
    "                    if content:\n",
    "                        return content.get_text(separator='\\n', strip=True)\n",
    "                \n",
    "                # If no content found with any selector\n",
    "                self.logger.warning(f\"No content found for {url}\")\n",
    "                return \"No content found\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error getting case text (attempt {attempt+1}): {e}\")\n",
    "                self.random_delay()\n",
    "        \n",
    "        # If all attempts failed\n",
    "        self.stats[\"errors\"] += 1\n",
    "        return \"Error retrieving content\"\n",
    "    \n",
    "    def extract_doc_id(self, url):\n",
    "        \"\"\"Extract the document ID from a URL\"\"\"\n",
    "        # URLs are typically in format: /doc/123456/\n",
    "        parts = url.strip('/').split('/')\n",
    "        for i, part in enumerate(parts):\n",
    "            if part == 'doc' and i+1 < len(parts):\n",
    "                return parts[i+1]\n",
    "        return None\n",
    "    \n",
    "    def get_cases_by_year_and_page(self, court_url, year, page):\n",
    "        \"\"\"Get cases for a specific court, year and page\"\"\"\n",
    "        cases = []\n",
    "        paginated_url = f\"{court_url}?year={year}&p={page}\"\n",
    "        self.logger.info(f\"Scraping: {paginated_url}\")\n",
    "        \n",
    "        for attempt in range(self.retry_limit):\n",
    "            try:\n",
    "                self.driver.get(paginated_url)\n",
    "                self.random_delay()\n",
    "                \n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                results = soup.find_all('div', class_='result')\n",
    "                \n",
    "                if not results:\n",
    "                    self.logger.info(f\"No results found for {court_url} in {year} page {page}\")\n",
    "                    return [], False  # No more pages\n",
    "                \n",
    "                for div in results:\n",
    "                    try:\n",
    "                        title_tag = div.find(\"a\")\n",
    "                        if not title_tag:\n",
    "                            continue\n",
    "                            \n",
    "                        title = title_tag.text.strip()\n",
    "                        url = self.base_url + title_tag['href']\n",
    "                        \n",
    "                        # Extract docid from URL\n",
    "                        docid = self.extract_doc_id(title_tag['href'])\n",
    "                        \n",
    "                        # Extract snippet\n",
    "                        snippet = div.find(\"p\", class_=\"snippet\")\n",
    "                        snippet_text = snippet.text.strip() if snippet else \"\"\n",
    "                        \n",
    "                        # Extract date\n",
    "                        date_span = div.find(\"span\", class_=\"result_date\")\n",
    "                        date_text = date_span.text.strip() if date_span else \"\"\n",
    "                        \n",
    "                        # Get court name from the result\n",
    "                        court_span = div.find(\"span\", class_=\"docsource\")\n",
    "                        court_name = court_span.text.strip() if court_span else \"\"\n",
    "                        \n",
    "                        self.logger.info(f\"Found case: {title} ({docid})\")\n",
    "                        \n",
    "                        case_data = {\n",
    "                            \"DocID\": docid,\n",
    "                            \"Title\": title,\n",
    "                            \"URL\": url,\n",
    "                            \"Snippet\": snippet_text,\n",
    "                            \"Date\": date_text,\n",
    "                            \"Year\": year,\n",
    "                            \"Court\": court_name\n",
    "                        }\n",
    "                        \n",
    "                        # Get full text if enabled\n",
    "                        if self.include_full_text:\n",
    "                            self.logger.debug(f\"Fetching full text for case {docid}\")\n",
    "                            case_data[\"Full_Text\"] = self.get_case_text(url)\n",
    "                        \n",
    "                        cases.append(case_data)\n",
    "                        self.stats[\"cases_scraped\"] += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Error processing case: {e}\")\n",
    "                        self.stats[\"errors\"] += 1\n",
    "                \n",
    "                return cases, True  # More pages may exist\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error scraping page (attempt {attempt+1}): {e}\")\n",
    "                self.random_delay()\n",
    "        \n",
    "        # If all attempts failed\n",
    "        self.stats[\"errors\"] += 1\n",
    "        return [], False\n",
    "    \n",
    "    def process_court_year(self, court_name, court_url, year):\n",
    "        \"\"\"Process all pages for a specific court and year\"\"\"\n",
    "        self.logger.info(f\"Processing {court_name} for year {year}\")\n",
    "        \n",
    "        # Set checkpoint data\n",
    "        self.checkpoint[\"current_court\"] = court_name\n",
    "        self.checkpoint[\"current_year\"] = year\n",
    "        \n",
    "        # Start from the saved page or 0\n",
    "        start_page = self.checkpoint.get(\"current_year_page\", 0)\n",
    "        \n",
    "        page = start_page\n",
    "        all_cases = []\n",
    "        has_more_pages = True\n",
    "        \n",
    "        while has_more_pages:\n",
    "            self.checkpoint[\"current_year_page\"] = page\n",
    "            self.save_checkpoint()\n",
    "            \n",
    "            cases, has_more_pages = self.get_cases_by_year_and_page(court_url, year, page)\n",
    "            all_cases.extend(cases)\n",
    "            \n",
    "            # Save periodically\n",
    "            if page % self.checkpoint_interval == 0 and all_cases:\n",
    "                safe_court_name = court_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "                self.save_to_csv(all_cases, f\"{safe_court_name}_{year}_partial_{page}\")\n",
    "                \n",
    "                # Log progress\n",
    "                elapsed = time.time() - self.stats[\"start_time\"]\n",
    "                self.logger.info(f\"Progress: {len(all_cases)} cases scraped for {court_name} {year} (elapsed: {elapsed:.1f}s)\")\n",
    "            \n",
    "            # Move to next page if more exist\n",
    "            if has_more_pages:\n",
    "                page += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Reset page counter for next year\n",
    "        self.checkpoint[\"current_year_page\"] = 0\n",
    "        \n",
    "        # Mark year as completed\n",
    "        if year not in self.checkpoint[\"current_court_years_completed\"]:\n",
    "            self.checkpoint[\"current_court_years_completed\"].append(year)\n",
    "            self.stats[\"years_completed\"] += 1\n",
    "        \n",
    "        self.save_checkpoint()\n",
    "        \n",
    "        # Save final results for this year\n",
    "        if all_cases:\n",
    "            safe_court_name = court_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "            self.save_to_csv(all_cases, f\"{safe_court_name}_{year}\")\n",
    "            self.logger.info(f\"Completed {court_name} for {year}: {len(all_cases)} cases\")\n",
    "        else:\n",
    "            self.logger.info(f\"No cases found for {court_name} in {year}\")\n",
    "        \n",
    "        return all_cases\n",
    "    \n",
    "    def process_court(self, court_name, court_url):\n",
    "        \"\"\"Process all years for a specific court\"\"\"\n",
    "        self.logger.info(f\"Processing court: {court_name}\")\n",
    "        \n",
    "        # Skip if court already completed\n",
    "        if court_name in self.checkpoint[\"courts_completed\"]:\n",
    "            self.logger.info(f\"Skipping completed court: {court_name}\")\n",
    "            return []\n",
    "        \n",
    "        # Set current court in checkpoint\n",
    "        self.checkpoint[\"current_court\"] = court_name\n",
    "        if not self.checkpoint.get(\"current_court_years_completed\"):\n",
    "            self.checkpoint[\"current_court_years_completed\"] = []\n",
    "        \n",
    "        all_court_cases = []\n",
    "        \n",
    "        # Process each year\n",
    "        years_to_process = range(self.start_year, self.end_year + 1)\n",
    "        for year in years_to_process:\n",
    "            # Skip if year already completed for this court\n",
    "            if year in self.checkpoint[\"current_court_years_completed\"]:\n",
    "                self.logger.info(f\"Skipping completed year {year} for {court_name}\")\n",
    "                continue\n",
    "                \n",
    "            yearly_cases = self.process_court_year(court_name, court_url, year)\n",
    "            all_court_cases.extend(yearly_cases)\n",
    "        \n",
    "        # Mark court as completed\n",
    "        self.checkpoint[\"courts_completed\"].append(court_name)\n",
    "        self.checkpoint[\"current_court_years_completed\"] = []\n",
    "        self.save_checkpoint()\n",
    "        \n",
    "        self.stats[\"courts_completed\"] += 1\n",
    "        \n",
    "        # Save all court results\n",
    "        if all_court_cases:\n",
    "            safe_court_name = court_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "            self.save_to_csv(all_court_cases, f\"{safe_court_name}_all\")\n",
    "            self.logger.info(f\"Completed court {court_name}: {len(all_court_cases)} total cases\")\n",
    "        \n",
    "        return all_court_cases\n",
    "    \n",
    "    def save_to_csv(self, cases, filename_prefix):\n",
    "        \"\"\"Save case data to CSV file\"\"\"\n",
    "        if not cases:\n",
    "            self.logger.warning(f\"No cases to save for {filename_prefix}\")\n",
    "            return\n",
    "            \n",
    "        # Create a clean filename\n",
    "        safe_name = filename_prefix.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        file_path = os.path.join(self.download_dir, f\"{safe_name}.csv\")\n",
    "        \n",
    "        self.logger.info(f\"Saving {len(cases)} cases to {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, \"w\", newline='', encoding='utf-8') as f:\n",
    "                # Get all possible keys from all dictionaries\n",
    "                fieldnames = set()\n",
    "                for case in cases:\n",
    "                    fieldnames.update(case.keys())\n",
    "                    \n",
    "                writer = csv.DictWriter(f, fieldnames=sorted(list(fieldnames)))\n",
    "                writer.writeheader()\n",
    "                writer.writerows(cases)\n",
    "                \n",
    "            self.logger.info(f\"Successfully saved to {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving to CSV: {e}\")\n",
    "            \n",
    "            # Try saving as JSON as backup\n",
    "            backup_path = os.path.join(self.download_dir, f\"{safe_name}_backup.json\")\n",
    "            try:\n",
    "                with open(backup_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(cases, f, ensure_ascii=False, indent=2)\n",
    "                self.logger.info(f\"Backup saved to {backup_path}\")\n",
    "            except Exception as e2:\n",
    "                self.logger.critical(f\"Failed to save backup: {e2}\")\n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"Print statistics about the scraping session\"\"\"\n",
    "        elapsed = time.time() - self.stats[\"start_time\"]\n",
    "        hours, remainder = divmod(elapsed, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        \n",
    "        self.logger.info(\"-\" * 40)\n",
    "        self.logger.info(\"SCRAPING STATISTICS\")\n",
    "        self.logger.info(\"-\" * 40)\n",
    "        self.logger.info(f\"Total cases scraped: {self.stats['cases_scraped']}\")\n",
    "        self.logger.info(f\"Courts completed: {self.stats['courts_completed']}\")\n",
    "        self.logger.info(f\"Years completed: {self.stats['years_completed']}\")\n",
    "        self.logger.info(f\"Errors encountered: {self.stats['errors']}\")\n",
    "        self.logger.info(f\"Total time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "        self.logger.info(f\"Cases per minute: {(self.stats['cases_scraped'] / (elapsed/60)):.2f}\")\n",
    "        self.logger.info(\"-\" * 40)\n",
    "    \n",
    "    def scrape_all(self):\n",
    "        \"\"\"Main function to scrape all courts and years\"\"\"\n",
    "        self.logger.info(\"Starting comprehensive scraping of Indian Kanoon\")\n",
    "        self.login()\n",
    "        \n",
    "        court_links = self.get_court_category_links()\n",
    "        if not court_links:\n",
    "            self.logger.critical(\"Failed to get court links. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        self.logger.info(f\"Will scrape {len(court_links)} courts from {self.start_year} to {self.end_year}\")\n",
    "        \n",
    "        # Process each court\n",
    "        for court_name, court_url in court_links.items():\n",
    "            self.process_court(court_name, court_url)\n",
    "        \n",
    "        self.logger.info(\"Scraping completed!\")\n",
    "        self.print_stats()\n",
    "        self.driver.quit()\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Comprehensive Indian Kanoon Scraper\")\n",
    "    parser.add_argument(\"--username\", \"-u\", help=\"Indian Kanoon username\")\n",
    "    parser.add_argument(\"--password\", \"-p\", help=\"Indian Kanoon password\")\n",
    "    parser.add_argument(\"--output\", \"-o\", default=\"indiankanoon-case-dump\", help=\"Output directory\")\n",
    "    parser.add_argument(\"--start-year\", \"-s\", type=int, default=1946, help=\"Start year\")\n",
    "    parser.add_argument(\"--end-year\", \"-e\", type=int, default=datetime.now().year, help=\"End year\")\n",
    "    parser.add_argument(\"--no-full-text\", action=\"store_true\", help=\"Skip downloading full text of cases\")\n",
    "    parser.add_argument(\"--delay-min\", type=float, default=1.0, help=\"Minimum delay between requests\")\n",
    "    parser.add_argument(\"--delay-max\", type=float, default=3.0, help=\"Maximum delay between requests\")\n",
    "    parser.add_argument(\"--checkpoint-interval\", type=int, default=10, help=\"Save checkpoint every N pages\")\n",
    "    parser.add_argument(\"--retry-limit\", type=int, default=3, help=\"Number of retry attempts for failed requests\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    \n",
    "    # Set up configuration\n",
    "    config = {\n",
    "        'base_url': \"https://indiankanoon.org\",\n",
    "        'download_dir': args.output,\n",
    "        'username': args.username,\n",
    "        'password': args.password,\n",
    "        'start_year': args.start_year,\n",
    "        'end_year': args.end_year,\n",
    "        'include_full_text': not args.no_full_text,\n",
    "        'delay_min': args.delay_min,\n",
    "        'delay_max': args.delay_max,\n",
    "        'checkpoint_interval': args.checkpoint_interval,\n",
    "        'retry_limit': args.retry_limit\n",
    "    }\n",
    "    \n",
    "    # Initialize and run scraper\n",
    "    scraper = IndianKanoonScraper(config)\n",
    "    scraper.scrape_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
